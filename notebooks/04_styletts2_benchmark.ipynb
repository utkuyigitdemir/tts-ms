{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# StyleTTS2 Benchmark\n\nBu notebook StyleTTS2 engine'ini 5 Türkçe mülakat sorusu ile test eder.\n\n**Engine:** StyleTTS2\n**GPU:** Gerekli (dinamik tespit)\n**Türkçe Desteği:** Sınırlı (İngilizce optimize)\n\n**NOT:** Bu notebook Miniconda ile Python 3.11 ortamı oluşturarak çalışır (versiyon uyumu için).\n\nHer soru için hem SORU hem CEVAP seslendiriliyor."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Mount Drive & Install Miniconda\nfrom google.colab import drive\ndrive.mount('/content/drive')\nprint(\"Drive mounted successfully\")\n\n# Install Miniconda\nimport os\nif not os.path.exists('/usr/local/bin/conda'):\n    print(\"\\nInstalling Miniconda...\")\n    !wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh\n    !bash /tmp/miniconda.sh -b -f -p /usr/local\n    !rm /tmp/miniconda.sh\n    print(\"Miniconda installed!\")\nelse:\n    print(\"Miniconda already installed\")\n\n!conda --version"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Create Python 3.11 Environment & Install Dependencies\nimport os\n\nENV_NAME = \"tts_py311\"\n\n# Accept Conda ToS (required for non-interactive environments)\nprint(\"Accepting Conda Terms of Service...\")\n!conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main 2>/dev/null || true\n!conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r 2>/dev/null || true\n\n# Create environment with conda-forge (avoid default channels)\nprint(\"\\nCreating Python 3.11 environment...\")\n!conda create -n $ENV_NAME python=3.11 -c conda-forge --override-channels -y -q\n\n# Install espeak-ng (required for StyleTTS2)\nprint(\"\\nInstalling espeak-ng...\")\n!apt-get install -y espeak-ng > /dev/null 2>&1\n\n# Install PyTorch with CUDA\nprint(\"\\nInstalling PyTorch...\")\n!source activate $ENV_NAME && pip install -q torch torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# Install StyleTTS2\nprint(\"\\nInstalling StyleTTS2...\")\n!source activate $ENV_NAME && pip install -q styletts2\n\n# Install monitoring tools\nprint(\"\\nInstalling monitoring tools...\")\n!source activate $ENV_NAME && pip install -q psutil pynvml\n\n# Verify installation\nprint(\"\\n\" + \"=\"*60)\nprint(\"VERIFICATION:\")\n!source activate $ENV_NAME && python --version\n!source activate $ENV_NAME && python -c \"import torch; print('PyTorch:', torch.__version__, 'CUDA:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A')\"\n!source activate $ENV_NAME && python -c \"import styletts2; print('StyleTTS2: OK')\"\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Setup Output Directories\nimport os\nimport shutil\n\nENGINE_NAME = \"styletts2\"\nMODEL_NAME = \"StyleTTS2\"\nENV_NAME = \"tts_py311\"\n\nBASE_DIR = f\"/content/drive/MyDrive/tts-ms/output/{ENGINE_NAME}\"\nAUDIO_DIR = f\"{BASE_DIR}/audio\"\n\n# Clean start\nif os.path.exists(BASE_DIR):\n    shutil.rmtree(BASE_DIR)\nos.makedirs(AUDIO_DIR, exist_ok=True)\nprint(f\"Output directory: {BASE_DIR}\")\n\n# Model cache on Drive\nMODEL_DIR = \"/content/drive/MyDrive/tts-ms/cache/styletts2\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nprint(f\"Model cache: {MODEL_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\nsource activate tts_py311\n\n# Fix matplotlib backend before any imports\nexport MPLBACKEND=agg\n\npython << 'EOF'\nimport os\nos.environ[\"MPLBACKEND\"] = \"agg\"  # Must be before matplotlib imports\n\nimport json\nimport time\nimport psutil\nimport torch\nfrom datetime import datetime\nfrom styletts2 import tts as styletts2_tts\n\n# GPU monitoring\ntry:\n    import pynvml\n    pynvml.nvmlInit()\n    GPU_HANDLE = pynvml.nvmlDeviceGetHandleByIndex(0)\n    GPU_AVAILABLE = True\n    GPU_NAME = pynvml.nvmlDeviceGetName(GPU_HANDLE)\n    if isinstance(GPU_NAME, bytes):\n        GPU_NAME = GPU_NAME.decode('utf-8')\nexcept:\n    GPU_AVAILABLE = False\n    GPU_NAME = \"N/A\"\n\n# Config\nENGINE_NAME = \"styletts2\"\nMODEL_NAME = \"StyleTTS2\"\nBASE_DIR = \"/content/drive/MyDrive/tts-ms/output/styletts2\"\nAUDIO_DIR = f\"{BASE_DIR}/audio\"\nMODEL_DIR = \"/content/drive/MyDrive/tts-ms/cache/styletts2\"\n\nos.environ[\"HF_HOME\"] = MODEL_DIR\n\nprocess = psutil.Process()\nCPU_COUNT = psutil.cpu_count()\nresource_logs = []\nresults = []\n\ndef get_gpu_stats():\n    if not GPU_AVAILABLE:\n        return {\"gpu_util\": 0, \"gpu_mem_used\": 0, \"gpu_mem_total\": 0}\n    try:\n        util = pynvml.nvmlDeviceGetUtilizationRates(GPU_HANDLE)\n        mem = pynvml.nvmlDeviceGetMemoryInfo(GPU_HANDLE)\n        return {\n            \"gpu_util\": util.gpu,\n            \"gpu_mem_used\": mem.used / 1024 / 1024,\n            \"gpu_mem_total\": mem.total / 1024 / 1024\n        }\n    except:\n        return {\"gpu_util\": 0, \"gpu_mem_used\": 0, \"gpu_mem_total\": 0}\n\ndef get_resources():\n    gpu = get_gpu_stats()\n    return {\n        \"cpu\": process.cpu_percent(),\n        \"ram_mb\": process.memory_info().rss / 1024 / 1024,\n        \"gpu_util\": gpu[\"gpu_util\"],\n        \"gpu_mem_used\": gpu[\"gpu_mem_used\"],\n        \"gpu_mem_total\": gpu[\"gpu_mem_total\"]\n    }\n\nQUESTIONS = [\n    {\"id\": \"01\", \"question\": \"Sizi neden işe almalıyız?\",\n     \"answer\": \"Güçlü analitik düşünme becerilerim ve takım çalışmasına yatkınlığım sayesinde projelere değer katabilirim. Ayrıca sürekli öğrenmeye açık yapım ve problem çözme yeteneklerim, şirketinizin hedeflerine ulaşmasında önemli katkılar sağlayacaktır.\"},\n    {\"id\": \"02\", \"question\": \"Siz bizi neden seçtiniz?\",\n     \"answer\": \"Şirketinizin yenilikçi yaklaşımı ve sektördeki lider konumu beni çok etkiledi. Kariyer hedeflerimle örtüşen bu ortamda kendimi geliştirebileceğime ve anlamlı projeler üzerinde çalışabileceğime inanıyorum.\"},\n    {\"id\": \"03\", \"question\": \"Kötü özellikleriniz nelerdir?\",\n     \"answer\": \"Bazen aşırı detaycı olabiliyorum, bu da zaman yönetimimi olumsuz etkileyebiliyor. Ancak bu özelliğimin farkındayım ve önceliklendirme teknikleri kullanarak bu durumu yönetmeye çalışıyorum.\"},\n    {\"id\": \"04\", \"question\": \"Beş yıl sonra kendinizi nerede görüyorsunuz?\",\n     \"answer\": \"Beş yıl içinde teknik liderlik pozisyonunda olmayı hedefliyorum. Ekip yönetimi deneyimi kazanarak şirketin büyümesine stratejik katkılar sağlamak istiyorum.\"},\n    {\"id\": \"05\", \"question\": \"Maaş beklentiniz nedir?\",\n     \"answer\": \"Piyasa koşullarını ve pozisyonun gerekliliklerini değerlendirerek, deneyimime ve yeteneklerime uygun rekabetçi bir maaş beklentim var. Bu konuda esnek olmaya ve karşılıklı bir anlaşmaya varmaya açığım.\"}\n]\n\nprint(\"=\"*60)\nprint(\"INITIALIZING STYLETTS2\")\nprint(\"=\"*60)\nprint(f\"GPU: {GPU_NAME}\")\n\nres_before = get_resources()\nstart = time.time()\n\ntts = styletts2_tts.StyleTTS2()\n\ninit_time = time.time() - start\nres_after = get_resources()\nresource_logs.append({\n    \"stage\": \"init\",\n    \"duration\": init_time,\n    \"cpu\": res_after[\"cpu\"],\n    \"ram_delta\": res_after[\"ram_mb\"] - res_before[\"ram_mb\"],\n    \"gpu_util\": res_after[\"gpu_util\"],\n    \"gpu_mem_delta\": res_after[\"gpu_mem_used\"] - res_before[\"gpu_mem_used\"]\n})\nprint(f\"Initialized in {init_time:.2f}s | GPU: {res_after['gpu_util']}% | VRAM: {res_after['gpu_mem_used']:.0f}MB\")\n\n# Warmup\nprint(\"\\nWarmup...\")\nres_before = get_resources()\nstart = time.time()\n_ = tts.inference(\"Hello.\", output_wav_file=f\"{AUDIO_DIR}/warmup.wav\")\nwarmup_time = time.time() - start\nres_after = get_resources()\nresource_logs.append({\n    \"stage\": \"warmup\",\n    \"text\": \"Hello.\",\n    \"duration\": warmup_time,\n    \"cpu\": res_after[\"cpu\"],\n    \"ram_delta\": res_after[\"ram_mb\"] - res_before[\"ram_mb\"],\n    \"gpu_util\": res_after[\"gpu_util\"],\n    \"gpu_mem_delta\": res_after[\"gpu_mem_used\"] - res_before[\"gpu_mem_used\"]\n})\nprint(f\"Warmup done in {warmup_time:.2f}s | GPU: {res_after['gpu_util']}% | VRAM: {res_after['gpu_mem_used']:.0f}MB\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"SYNTHESIZING {len(QUESTIONS)} QUESTIONS + ANSWERS\")\nprint(\"=\"*60)\n\nfor q in QUESTIONS:\n    print(f\"\\n[{q['id']}] {q['question']}\")\n    \n    for typ, text in [(\"soru\", q[\"question\"]), (\"cevap\", q[\"answer\"])]:\n        print(f\"  {typ.upper()}: \", end=\"\", flush=True)\n        res_before = get_resources()\n        start = time.time()\n        try:\n            path = f\"{AUDIO_DIR}/{q['id']}_{typ}.wav\"\n            tts.inference(text, output_wav_file=path)\n            elapsed = time.time() - start\n            res_after = get_resources()\n            \n            size = os.path.getsize(path)\n            cpu_norm = res_after[\"cpu\"] / CPU_COUNT\n            ram_delta = res_after[\"ram_mb\"] - res_before[\"ram_mb\"]\n            gpu_util = res_after[\"gpu_util\"]\n            gpu_mem_delta = res_after[\"gpu_mem_used\"] - res_before[\"gpu_mem_used\"]\n            \n            resource_logs.append({\n                \"stage\": f\"{q['id']}_{typ}\",\n                \"text\": text,\n                \"duration\": elapsed,\n                \"cpu\": res_after[\"cpu\"],\n                \"cpu_norm\": cpu_norm,\n                \"ram_delta\": ram_delta,\n                \"gpu_util\": gpu_util,\n                \"gpu_mem_used\": res_after[\"gpu_mem_used\"],\n                \"gpu_mem_delta\": gpu_mem_delta,\n                \"size_kb\": size/1024\n            })\n            results.append({\n                \"id\": q[\"id\"],\n                \"type\": typ,\n                \"text\": text,\n                \"time\": elapsed,\n                \"size\": size,\n                \"cpu\": cpu_norm,\n                \"ram_delta\": ram_delta,\n                \"gpu_util\": gpu_util,\n                \"gpu_mem_delta\": gpu_mem_delta,\n                \"status\": \"OK\"\n            })\n            \n            print(f\"{elapsed:.2f}s | {size/1024:.1f}KB | CPU:{cpu_norm:.0f}% | GPU:{gpu_util}% | OK\")\n        except Exception as e:\n            results.append({\n                \"id\": q[\"id\"],\n                \"type\": typ,\n                \"text\": text,\n                \"time\": time.time()-start,\n                \"size\": 0,\n                \"cpu\": 0,\n                \"ram_delta\": 0,\n                \"gpu_util\": 0,\n                \"gpu_mem_delta\": 0,\n                \"status\": f\"FAIL: {e}\"\n            })\n            print(f\"FAIL: {e}\")\n\nsuccessful = [r for r in results if r[\"status\"] == \"OK\"]\nprint(f\"\\n\" + \"=\"*60)\nprint(f\"COMPLETE: {len(successful)}/{len(results)} successful\")\nprint(\"=\"*60)\n\n# Save results\noutput_data = {\n    \"engine\": ENGINE_NAME,\n    \"model\": MODEL_NAME,\n    \"gpu_name\": GPU_NAME,\n    \"init_time\": init_time,\n    \"warmup_time\": warmup_time,\n    \"results\": results,\n    \"resource_logs\": resource_logs,\n    \"timestamp\": datetime.now().isoformat()\n}\n\nwith open(f\"{BASE_DIR}/results.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(output_data, f, indent=2, ensure_ascii=False)\n\nprint(f\"\\nResults saved to {BASE_DIR}/results.json\")\nEOF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Play Audio Files\nfrom IPython.display import Audio, display\nfrom pathlib import Path\n\nENGINE_NAME = \"styletts2\"\nAUDIO_DIR = f\"/content/drive/MyDrive/tts-ms/output/{ENGINE_NAME}/audio\"\n\nprint(\"=\"*60)\nprint(\"AUDIO PLAYBACK\")\nprint(\"=\"*60)\n\naudio_files = sorted(Path(AUDIO_DIR).glob(\"*.wav\"))\naudio_files = [f for f in audio_files if f.name != \"warmup.wav\"]\n\nif not audio_files:\n    print(\"No audio files found!\")\nelse:\n    for wav in audio_files:\n        print(f\"\\n{wav.name}:\")\n        display(Audio(str(wav)))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Generate Reports\nimport json\nimport psutil\nfrom datetime import datetime\n\nENGINE_NAME = \"styletts2\"\nMODEL_NAME = \"StyleTTS2\"\nBASE_DIR = f\"/content/drive/MyDrive/tts-ms/output/{ENGINE_NAME}\"\nCPU_COUNT = psutil.cpu_count()\n\n# Load results\nwith open(f\"{BASE_DIR}/results.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\nresults = data[\"results\"]\nresource_logs = data[\"resource_logs\"]\ninit_time = data[\"init_time\"]\nwarmup_time = data[\"warmup_time\"]\ngpu_name = data.get(\"gpu_name\", \"N/A\")\n\nsuccessful = [r for r in results if r[\"status\"] == \"OK\"]\nsoru_results = [r for r in successful if r[\"type\"] == \"soru\"]\ncevap_results = [r for r in successful if r[\"type\"] == \"cevap\"]\n\ntotal_size_kb = sum(r[\"size\"] for r in successful) / 1024\navg_soru = sum(r[\"time\"] for r in soru_results) / len(soru_results) if soru_results else 0\navg_cevap = sum(r[\"time\"] for r in cevap_results) / len(cevap_results) if cevap_results else 0\n\nsynth_logs = [r for r in resource_logs if r[\"stage\"] not in [\"init\", \"warmup\"]]\navg_cpu = sum(r.get(\"cpu_norm\", 0) for r in synth_logs) / len(synth_logs) if synth_logs else 0\nmax_cpu = max(r.get(\"cpu_norm\", 0) for r in synth_logs) if synth_logs else 0\ntotal_ram = sum(r.get(\"ram_delta\", 0) for r in synth_logs)\n\n# GPU metrics\navg_gpu = sum(r.get(\"gpu_util\", 0) for r in synth_logs) / len(synth_logs) if synth_logs else 0\nmax_gpu = max(r.get(\"gpu_util\", 0) for r in synth_logs) if synth_logs else 0\nmax_vram = max(r.get(\"gpu_mem_used\", 0) for r in synth_logs) if synth_logs else 0\n\n# summary.txt\nsummary = f\"\"\"============================================================\nTTS BENCHMARK - STYLETTS2\n============================================================\nTarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\nPlatform: Google Colab (Miniconda Python 3.11)\nGPU: {gpu_name}\nModel: {MODEL_NAME}\n\nPERFORMANS:\n- Init: {init_time:.2f}s\n- Warmup: {warmup_time:.2f}s\n\nSONUCLAR:\n- Toplam: 5 soru\n- Soru Audio: {len(soru_results)}/5 basarili (ort: {avg_soru:.2f}s)\n- Cevap Audio: {len(cevap_results)}/5 basarili (ort: {avg_cevap:.2f}s)\n- Toplam Audio: {total_size_kb:.1f} KB\n\nKAYNAK KULLANIMI:\n- Ortalama CPU: {avg_cpu:.0f}%\n- Maksimum CPU: {max_cpu:.0f}%\n- RAM Delta: {total_ram:+.1f} MB\n- Ortalama GPU: {avg_gpu:.0f}%\n- Maksimum GPU: {max_gpu:.0f}%\n- Maksimum VRAM: {max_vram:.0f} MB\n\nDETAYLAR:\n\"\"\"\nfor r in results:\n    s = \"OK\" if r[\"status\"] == \"OK\" else \"FAIL\"\n    text_preview = r.get(\"text\", \"\")[:50] + \"...\" if len(r.get(\"text\", \"\")) > 50 else r.get(\"text\", \"\")\n    summary += f\"[{r['id']}_{r['type']}] {r['time']:.2f}s | {r['size']/1024:.1f} KB | CPU:{r['cpu']:.0f}% | GPU:{r.get('gpu_util', 0)}% | {s}\\n\"\n    summary += f\"    Text: {text_preview}\\n\"\nsummary += f\"\\nAudio: {BASE_DIR}/audio\\n============================================================\\n\"\n\nwith open(f\"{BASE_DIR}/summary.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(summary)\n\nprint(summary)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Download ZIP\nimport zipfile\nfrom pathlib import Path\nfrom google.colab import files\n\nENGINE_NAME = \"styletts2\"\nBASE_DIR = f\"/content/drive/MyDrive/tts-ms/output/{ENGINE_NAME}\"\nAUDIO_DIR = f\"{BASE_DIR}/audio\"\n\nzip_path = f\"{BASE_DIR}/styletts2_benchmark.zip\"\n\nwith zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    zf.write(f\"{BASE_DIR}/summary.txt\", \"summary.txt\")\n    zf.write(f\"{BASE_DIR}/results.json\", \"results.json\")\n    for wav in Path(AUDIO_DIR).glob(\"*.wav\"):\n        if wav.name != \"warmup.wav\":\n            zf.write(wav, f\"audio/{wav.name}\")\n\nprint(f\"ZIP: {zip_path}\")\nprint(f\"Size: {Path(zip_path).stat().st_size/1024:.1f} KB\")\nprint(\"\\nContents:\")\nwith zipfile.ZipFile(zip_path, \"r\") as zf:\n    for f in zf.namelist():\n        print(f\"  {f}\")\n\nfiles.download(zip_path)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}