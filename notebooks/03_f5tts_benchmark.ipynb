{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# F5-TTS Benchmark\n\nBu notebook F5-TTS engine'ini 5 Türkçe mülakat sorusu ile test eder.\n\n**Engine:** F5-TTS\n**GPU:** Gerekli (dinamik tespit)\n**Türkçe Desteği:** Evet (multilingual)\n\n**NOT:** Bu notebook Miniconda ile Python 3.11 ortamı oluşturarak çalışır (versiyon uyumu için).\n\nHer soru için hem SORU hem CEVAP seslendiriliyor."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Mount Drive & Install Miniconda\nfrom google.colab import drive\ndrive.mount('/content/drive')\nprint(\"Drive mounted successfully\")\n\n# Install Miniconda\nimport os\nif not os.path.exists('/usr/local/bin/conda'):\n    print(\"\\nInstalling Miniconda...\")\n    !wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh\n    !bash /tmp/miniconda.sh -b -f -p /usr/local\n    !rm /tmp/miniconda.sh\n    print(\"Miniconda installed!\")\nelse:\n    print(\"Miniconda already installed\")\n\n!conda --version"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Create Python 3.11 Environment & Install Dependencies\nimport os\n\nENV_NAME = \"tts_py311\"\n\n# Accept Conda ToS (required for non-interactive environments)\nprint(\"Accepting Conda Terms of Service...\")\n!conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main 2>/dev/null || true\n!conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r 2>/dev/null || true\n\n# Create environment with conda-forge (avoid default channels)\nprint(\"\\nCreating Python 3.11 environment...\")\n!conda create -n $ENV_NAME python=3.11 -c conda-forge --override-channels -y -q\n\n# Install ffmpeg (required for audio processing)\nprint(\"\\nInstalling ffmpeg...\")\n!apt-get install -y ffmpeg > /dev/null 2>&1\n\n# Install PyTorch with CUDA\nprint(\"\\nInstalling PyTorch...\")\n!source activate $ENV_NAME && pip install -q torch torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# Install F5-TTS\nprint(\"\\nInstalling F5-TTS...\")\n!source activate $ENV_NAME && pip install -q f5-tts\n\n# Install edge-tts (high-quality Microsoft Neural TTS for reference audio)\nprint(\"\\nInstalling edge-tts...\")\n!source activate $ENV_NAME && pip install -q edge-tts\n\n# Install monitoring tools\nprint(\"\\nInstalling monitoring tools...\")\n!source activate $ENV_NAME && pip install -q psutil pynvml\n\n# Verify installation\nprint(\"\\n\" + \"=\"*60)\nprint(\"VERIFICATION:\")\n!source activate $ENV_NAME && python --version\n!source activate $ENV_NAME && python -c \"import torch; print('PyTorch:', torch.__version__, 'CUDA:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A')\"\n!source activate $ENV_NAME && python -c \"import f5_tts; print('F5-TTS: OK')\"\n!source activate $ENV_NAME && python -c \"import edge_tts; print('edge-tts: OK')\"\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Setup Output Directories\nimport os\nimport shutil\n\nENGINE_NAME = \"f5tts\"\nMODEL_NAME = \"F5-TTS\"\nENV_NAME = \"tts_py311\"\n\nBASE_DIR = f\"/content/drive/MyDrive/tts-ms/output/{ENGINE_NAME}\"\nAUDIO_DIR = f\"{BASE_DIR}/audio\"\n\n# Clean start\nif os.path.exists(BASE_DIR):\n    shutil.rmtree(BASE_DIR)\nos.makedirs(AUDIO_DIR, exist_ok=True)\nprint(f\"Output directory: {BASE_DIR}\")\n\n# Model cache on Drive\nMODEL_DIR = \"/content/drive/MyDrive/tts-ms/cache/f5tts\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nprint(f\"Model cache: {MODEL_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\nsource activate tts_py311\n\nexport MPLBACKEND=agg\nexport LANG=en_US.UTF-8\nexport LC_ALL=en_US.UTF-8\nexport PYTHONIOENCODING=utf-8\n\n# Generate high-quality Turkish reference audio with edge-tts\nREF_DIR=\"/content/drive/MyDrive/tts-ms/cache/f5tts/ref\"\nmkdir -p \"$REF_DIR\"\nrm -f \"$REF_DIR\"/*.wav \"$REF_DIR\"/*.mp3 2>/dev/null\n\necho \"Generating Turkish reference audio with edge-tts (Microsoft Neural TTS)...\"\nedge-tts --voice \"tr-TR-EmelNeural\" \\\n    --text \"Merhaba, ben bir Türk ses asistanıyım. Bugün sizinle görüşmekten mutluluk duyuyorum.\" \\\n    --write-media \"$REF_DIR/turkish_ref.mp3\"\n\n# Convert to WAV\nffmpeg -y -i \"$REF_DIR/turkish_ref.mp3\" -ar 24000 -ac 1 \"$REF_DIR/turkish_ref.wav\" 2>/dev/null\necho \"Reference audio ready: $REF_DIR/turkish_ref.wav\"\n\npython -u << 'ENDOFPYTHON'\n# -*- coding: utf-8 -*-\nimport os\nos.environ[\"MPLBACKEND\"] = \"agg\"\n\nimport json\nimport time\nimport psutil\nimport torch\nimport torchaudio\nfrom datetime import datetime\nfrom f5_tts.api import F5TTS\n\n# GPU monitoring\ntry:\n    import pynvml\n    pynvml.nvmlInit()\n    GPU_HANDLE = pynvml.nvmlDeviceGetHandleByIndex(0)\n    GPU_AVAILABLE = True\n    GPU_NAME = pynvml.nvmlDeviceGetName(GPU_HANDLE)\n    if isinstance(GPU_NAME, bytes):\n        GPU_NAME = GPU_NAME.decode('utf-8')\nexcept:\n    GPU_AVAILABLE = False\n    GPU_NAME = \"N/A\"\n\n# Config\nENGINE_NAME = \"f5tts\"\nMODEL_NAME = \"F5-TTS\"\nBASE_DIR = \"/content/drive/MyDrive/tts-ms/output/f5tts\"\nAUDIO_DIR = f\"{BASE_DIR}/audio\"\nMODEL_DIR = \"/content/drive/MyDrive/tts-ms/cache/f5tts\"\n\nos.environ[\"HF_HOME\"] = MODEL_DIR\n\n# Reference audio (generated by edge-tts above)\nREF_AUDIO = f\"{MODEL_DIR}/ref/turkish_ref.wav\"\nREF_TEXT = \"Merhaba, ben bir Türk ses asistanıyım. Bugün sizinle görüşmekten mutluluk duyuyorum.\"\n\n# Verify\nwaveform, sr = torchaudio.load(REF_AUDIO)\nprint(f\"Reference: {waveform.shape[1]/sr:.1f}s, {sr}Hz\")\n\nprocess = psutil.Process()\nCPU_COUNT = psutil.cpu_count()\nresource_logs = []\nresults = []\n\ndef get_gpu_stats():\n    if not GPU_AVAILABLE:\n        return {\"gpu_util\": 0, \"gpu_mem_used\": 0, \"gpu_mem_total\": 0}\n    try:\n        util = pynvml.nvmlDeviceGetUtilizationRates(GPU_HANDLE)\n        mem = pynvml.nvmlDeviceGetMemoryInfo(GPU_HANDLE)\n        return {\"gpu_util\": util.gpu, \"gpu_mem_used\": mem.used/1024/1024, \"gpu_mem_total\": mem.total/1024/1024}\n    except:\n        return {\"gpu_util\": 0, \"gpu_mem_used\": 0, \"gpu_mem_total\": 0}\n\ndef get_resources():\n    gpu = get_gpu_stats()\n    return {\n        \"cpu\": process.cpu_percent(),\n        \"ram_mb\": process.memory_info().rss/1024/1024,\n        \"gpu_util\": gpu[\"gpu_util\"],\n        \"gpu_mem_used\": gpu[\"gpu_mem_used\"],\n        \"gpu_mem_total\": gpu[\"gpu_mem_total\"]\n    }\n\nQUESTIONS = [\n    {\"id\": \"01\", \"question\": \"Sizi neden işe almalıyız?\",\n     \"answer\": \"Güçlü analitik düşünme becerilerim ve takım çalışmasına yatkınlığım sayesinde projelere değer katabilirim. Ayrıca sürekli öğrenmeye açık yapım ve problem çözme yeteneklerim, şirketinizin hedeflerine ulaşmasında önemli katkılar sağlayacaktır.\"},\n    {\"id\": \"02\", \"question\": \"Siz bizi neden seçtiniz?\",\n     \"answer\": \"Şirketinizin yenilikçi yaklaşımı ve sektördeki lider konumu beni çok etkiledi. Kariyer hedeflerimle örtüşen bu ortamda kendimi geliştirebileceğime ve anlamlı projeler üzerinde çalışabileceğime inanıyorum.\"},\n    {\"id\": \"03\", \"question\": \"Kötü özellikleriniz nelerdir?\",\n     \"answer\": \"Bazen aşırı detaycı olabiliyorum, bu da zaman yönetimimi olumsuz etkileyebiliyor. Ancak bu özelliğimin farkındayım ve önceliklendirme teknikleri kullanarak bu durumu yönetmeye çalışıyorum.\"},\n    {\"id\": \"04\", \"question\": \"Beş yıl sonra kendinizi nerede görüyorsunuz?\",\n     \"answer\": \"Beş yıl içinde teknik liderlik pozisyonunda olmayı hedefliyorum. Ekip yönetimi deneyimi kazanarak şirketin büyümesine stratejik katkılar sağlamak istiyorum.\"},\n    {\"id\": \"05\", \"question\": \"Maaş beklentiniz nedir?\",\n     \"answer\": \"Piyasa koşullarını ve pozisyonun gerekliliklerini değerlendirerek, deneyimime ve yeteneklerime uygun rekabetçi bir maaş beklentim var. Bu konuda esnek olmaya ve karşılıklı bir anlaşmaya varmaya açığım.\"}\n]\n\nprint(\"=\"*60)\nprint(\"INITIALIZING F5-TTS\")\nprint(\"=\"*60)\nprint(f\"GPU: {GPU_NAME}\")\n\nres_before = get_resources()\nstart = time.time()\ntts = F5TTS(device=\"cuda\")\ninit_time = time.time() - start\nres_after = get_resources()\nresource_logs.append({\"stage\": \"init\", \"duration\": init_time, \"cpu\": res_after[\"cpu\"], \"ram_delta\": res_after[\"ram_mb\"]-res_before[\"ram_mb\"], \"gpu_util\": res_after[\"gpu_util\"], \"gpu_mem_delta\": res_after[\"gpu_mem_used\"]-res_before[\"gpu_mem_used\"]})\nprint(f\"Initialized in {init_time:.2f}s | VRAM: {res_after['gpu_mem_used']:.0f}MB\")\n\nprint(\"\\nWarmup...\")\nres_before = get_resources()\nstart = time.time()\naudio, sr, _ = tts.infer(ref_file=REF_AUDIO, ref_text=REF_TEXT, gen_text=\"Merhaba, nasılsınız?\")\ntorchaudio.save(f\"{AUDIO_DIR}/warmup.wav\", torch.tensor(audio).unsqueeze(0), sr)\nwarmup_time = time.time() - start\nres_after = get_resources()\nresource_logs.append({\"stage\": \"warmup\", \"duration\": warmup_time, \"cpu\": res_after[\"cpu\"], \"ram_delta\": res_after[\"ram_mb\"]-res_before[\"ram_mb\"], \"gpu_util\": res_after[\"gpu_util\"], \"gpu_mem_delta\": res_after[\"gpu_mem_used\"]-res_before[\"gpu_mem_used\"]})\nprint(f\"Warmup done in {warmup_time:.2f}s\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"SYNTHESIZING {len(QUESTIONS)} QUESTIONS + ANSWERS\")\nprint(\"=\"*60)\n\nfor q in QUESTIONS:\n    print(f\"\\n[{q['id']}] {q['question']}\")\n    for typ, text in [(\"soru\", q[\"question\"]), (\"cevap\", q[\"answer\"])]:\n        print(f\"  {typ.upper()}: \", end=\"\", flush=True)\n        res_before = get_resources()\n        start = time.time()\n        try:\n            path = f\"{AUDIO_DIR}/{q['id']}_{typ}.wav\"\n            audio, sr, _ = tts.infer(ref_file=REF_AUDIO, ref_text=REF_TEXT, gen_text=text)\n            torchaudio.save(path, torch.tensor(audio).unsqueeze(0), sr)\n            elapsed = time.time() - start\n            res_after = get_resources()\n            size = os.path.getsize(path)\n            cpu_norm = res_after[\"cpu\"] / CPU_COUNT\n            \n            resource_logs.append({\"stage\": f\"{q['id']}_{typ}\", \"text\": text, \"duration\": elapsed, \"cpu\": res_after[\"cpu\"], \"cpu_norm\": cpu_norm, \"ram_delta\": res_after[\"ram_mb\"]-res_before[\"ram_mb\"], \"gpu_util\": res_after[\"gpu_util\"], \"gpu_mem_used\": res_after[\"gpu_mem_used\"], \"size_kb\": size/1024})\n            results.append({\"id\": q[\"id\"], \"type\": typ, \"text\": text, \"time\": elapsed, \"size\": size, \"cpu\": cpu_norm, \"ram_delta\": res_after[\"ram_mb\"]-res_before[\"ram_mb\"], \"gpu_util\": res_after[\"gpu_util\"], \"status\": \"OK\"})\n            print(f\"{elapsed:.2f}s | {size/1024:.1f}KB | GPU:{res_after['gpu_util']}% | OK\")\n        except Exception as e:\n            results.append({\"id\": q[\"id\"], \"type\": typ, \"text\": text, \"time\": time.time()-start, \"size\": 0, \"cpu\": 0, \"ram_delta\": 0, \"gpu_util\": 0, \"status\": f\"FAIL: {e}\"})\n            print(f\"FAIL: {e}\")\n\nsuccessful = [r for r in results if r[\"status\"] == \"OK\"]\nprint(f\"\\n{'='*60}\\nCOMPLETE: {len(successful)}/{len(results)} successful\\n{'='*60}\")\n\noutput_data = {\"engine\": ENGINE_NAME, \"model\": MODEL_NAME, \"gpu_name\": GPU_NAME, \"init_time\": init_time, \"warmup_time\": warmup_time, \"results\": results, \"resource_logs\": resource_logs, \"timestamp\": datetime.now().isoformat()}\nwith open(f\"{BASE_DIR}/results.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(output_data, f, indent=2, ensure_ascii=False)\nprint(f\"\\nResults saved to {BASE_DIR}/results.json\")\nENDOFPYTHON"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Play Audio Files\nfrom IPython.display import Audio, display\nfrom pathlib import Path\n\nENGINE_NAME = \"f5tts\"\nAUDIO_DIR = f\"/content/drive/MyDrive/tts-ms/output/{ENGINE_NAME}/audio\"\n\nprint(\"=\"*60)\nprint(\"AUDIO PLAYBACK\")\nprint(\"=\"*60)\n\naudio_files = sorted(Path(AUDIO_DIR).glob(\"*.wav\"))\naudio_files = [f for f in audio_files if f.name != \"warmup.wav\"]\n\nif not audio_files:\n    print(\"No audio files found!\")\nelse:\n    for wav in audio_files:\n        print(f\"\\n{wav.name}:\")\n        display(Audio(str(wav)))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Generate Reports\nimport json\nimport psutil\nfrom datetime import datetime\n\nENGINE_NAME = \"f5tts\"\nMODEL_NAME = \"F5-TTS\"\nBASE_DIR = f\"/content/drive/MyDrive/tts-ms/output/{ENGINE_NAME}\"\nCPU_COUNT = psutil.cpu_count()\n\n# Load results\nwith open(f\"{BASE_DIR}/results.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\nresults = data[\"results\"]\nresource_logs = data[\"resource_logs\"]\ninit_time = data[\"init_time\"]\nwarmup_time = data[\"warmup_time\"]\ngpu_name = data.get(\"gpu_name\", \"Unknown GPU\")\n\nsuccessful = [r for r in results if r[\"status\"] == \"OK\"]\nsoru_results = [r for r in successful if r[\"type\"] == \"soru\"]\ncevap_results = [r for r in successful if r[\"type\"] == \"cevap\"]\n\ntotal_size_kb = sum(r[\"size\"] for r in successful) / 1024\navg_soru = sum(r[\"time\"] for r in soru_results) / len(soru_results) if soru_results else 0\navg_cevap = sum(r[\"time\"] for r in cevap_results) / len(cevap_results) if cevap_results else 0\n\nsynth_logs = [r for r in resource_logs if r[\"stage\"] not in [\"init\", \"warmup\"]]\navg_cpu = sum(r.get(\"cpu_norm\", 0) for r in synth_logs) / len(synth_logs) if synth_logs else 0\nmax_cpu = max(r.get(\"cpu_norm\", 0) for r in synth_logs) if synth_logs else 0\ntotal_ram = sum(r.get(\"ram_delta\", 0) for r in synth_logs)\navg_gpu = sum(r.get(\"gpu_util\", 0) for r in synth_logs) / len(synth_logs) if synth_logs else 0\nmax_gpu = max(r.get(\"gpu_util\", 0) for r in synth_logs) if synth_logs else 0\nmax_vram = max(r.get(\"gpu_mem_used\", 0) for r in synth_logs) if synth_logs else 0\n\n# summary.txt\nsummary = f\"\"\"============================================================\nTTS BENCHMARK - F5-TTS\n============================================================\nTarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\nPlatform: Google Colab (Miniconda Python 3.11)\nGPU: {gpu_name}\nModel: {MODEL_NAME}\n\nPERFORMANS:\n- Init: {init_time:.2f}s\n- Warmup: {warmup_time:.2f}s\n\nSONUCLAR:\n- Toplam: 5 soru\n- Soru Audio: {len(soru_results)}/5 basarili (ort: {avg_soru:.2f}s)\n- Cevap Audio: {len(cevap_results)}/5 basarili (ort: {avg_cevap:.2f}s)\n- Toplam Audio: {total_size_kb:.1f} KB\n\nKAYNAK KULLANIMI:\n- CPU: Ort {avg_cpu:.0f}% / Max {max_cpu:.0f}%\n- RAM Delta: {total_ram:+.1f} MB\n- GPU: Ort {avg_gpu:.0f}% / Max {max_gpu:.0f}%\n- VRAM Max: {max_vram:.0f} MB\n\nDETAYLAR:\n\"\"\"\nfor r in results:\n    s = \"OK\" if r[\"status\"] == \"OK\" else \"FAIL\"\n    text_preview = r[\"text\"][:50] + \"...\" if len(r[\"text\"]) > 50 else r[\"text\"]\n    summary += f\"[{r['id']}_{r['type']}] {r['time']:.2f}s | {r['size']/1024:.1f}KB | CPU:{r['cpu']:.0f}% | GPU:{r.get('gpu_util',0)}% | {s}\\n\"\n    summary += f\"    Text: {text_preview}\\n\"\nsummary += f\"\\nAudio: {BASE_DIR}/audio\\n============================================================\\n\"\n\nwith open(f\"{BASE_DIR}/summary.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(summary)\n\nprint(summary)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Download ZIP\nimport zipfile\nfrom pathlib import Path\nfrom google.colab import files\n\nENGINE_NAME = \"f5tts\"\nBASE_DIR = f\"/content/drive/MyDrive/tts-ms/output/{ENGINE_NAME}\"\nAUDIO_DIR = f\"{BASE_DIR}/audio\"\n\nzip_path = f\"{BASE_DIR}/f5tts_benchmark.zip\"\n\nwith zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    zf.write(f\"{BASE_DIR}/summary.txt\", \"summary.txt\")\n    zf.write(f\"{BASE_DIR}/results.json\", \"results.json\")\n    for wav in Path(AUDIO_DIR).glob(\"*.wav\"):\n        if wav.name != \"warmup.wav\":\n            zf.write(wav, f\"audio/{wav.name}\")\n\nprint(f\"ZIP: {zip_path}\")\nprint(f\"Size: {Path(zip_path).stat().st_size/1024:.1f} KB\")\nprint(\"\\nContents:\")\nwith zipfile.ZipFile(zip_path, \"r\") as zf:\n    for f in zf.namelist():\n        print(f\"  {f}\")\n\nfiles.download(zip_path)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}